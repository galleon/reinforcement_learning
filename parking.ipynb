{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parking.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ej6aSTB9sOn7"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZUOLel5Gd+cXGr+y7/+n6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackdarkside/reinforcement_learning/blob/master/parking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ22Yy_1sGzK",
        "colab_type": "text"
      },
      "source": [
        "# Install Dependency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh4K-75nWa42",
        "colab_type": "text"
      },
      "source": [
        "install dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0rYjYvOVNsp",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "af6356f1-6fb8-4212-82a8-3b504993305c"
      },
      "source": [
        "%tensorflow_version 1.14.0 > /dev/null 2>&1\n",
        "!pip install stable-baselines[mpi]==2.10.0 > /dev/null 2>&1\n",
        "!pip install git+https://github.com/eleurent/highway-env > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14.0 > /dev/null 2>&1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXs4Zi8PWdes",
        "colab_type": "text"
      },
      "source": [
        "import dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-J7n6uDWUqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "import highway_env\n",
        "import numpy as np\n",
        "from stable_baselines import HER, SAC, DDPG\n",
        "from stable_baselines.ddpg import NormalActionNoise\n",
        "gymlogger.set_level(40) #error only\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldIq6QfNi68k",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgIx7ScZcKZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HYPERPARAMS = {\n",
        " 'parking_sac': {\n",
        "      'env_name': \"parking-v0\",\n",
        "      'tb_log_name': \"SAC_first_run\",\n",
        "      'filename': \"her_sac_highway\",\n",
        "      'learning_rate': 1e-3,\n",
        "      'buffer_size': int(1e6),\n",
        "      'gamma': 0.95,\n",
        "      'verbose': 1,\n",
        "      'batch_size': 256,\n",
        "      'goal_selection_strategy': 'future',\n",
        "      'tensorboard_log': \"./tensorboard/\",\n",
        "      'n_sampled_goal': 4,\n",
        "      'steps': int(1e5)\n",
        " },\n",
        " 'parking_ddpg': {\n",
        "      'env_name': \"parking-v0\",\n",
        "      'noise_std': 0.2,\n",
        "      'tb_log_name': \"DDPG_first_run\",\n",
        "      'filename': \"her_ddpg_highway\",\n",
        "      'gamma': 0.95,\n",
        "      'batch_size': 256,\n",
        "      'verbose': 1,\n",
        "      'actor_lr': 1e-3,\n",
        "      'critic_lr': 1e-3,\n",
        "      'buffer_size': int(1e6),\n",
        "      'goal_selection_strategy': 'future',\n",
        "      'tensorboard_log': \"./tensorboard/\",\n",
        "      'n_sampled_goal': 4,\n",
        "      'steps': int(2e5)\n",
        "\n",
        " }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAm1G27UsMFF",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVOWlPdV0zb",
        "colab_type": "text"
      },
      "source": [
        "SAC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV-0asPty7DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = HYPERPARAMS[\"parking_sac\"]\n",
        "\n",
        "env = gym.make(params[\"env_name\"])\n",
        "model = HER('MlpPolicy', env, SAC,\n",
        "            n_sampled_goal=params[\"n_sampled_goal\"],\n",
        "            goal_selection_strategy=params[\"goal_selection_strategy\"],\n",
        "            verbose=params[\"verbose\"],\n",
        "            tensorboard_log=params[\"tensorboard_log\"],\n",
        "            buffer_size=params[\"buffer_size\"],\n",
        "            learning_rate=params[\"learning_rate\"],\n",
        "            gamma=params[\"gamma\"],\n",
        "            batch_size=params[\"batch_size\"],\n",
        "            policy_kwargs=dict(layers=[256, 256, 256]))\n",
        "\n",
        "print(\"start training\")\n",
        "model.learn(params[\"steps\"], tb_log_name=params[\"tb_log_name\"])\n",
        "model.save(params[\"filename\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPCAEYfYV0J4",
        "colab_type": "text"
      },
      "source": [
        "DDPG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGmoBv7eV3h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = HYPERPARAMS[\"parking_ddpg\"]\n",
        "\n",
        "env = gym.make(params[\"env_name\"])\n",
        "n_actions = env.action_space.shape[0]\n",
        "noise_std = params[\"noise_std\"]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions))\n",
        "\n",
        "model = HER('MlpPolicy', env, DDPG,\n",
        "            n_sampled_goal=params[\"n_sampled_goal\"],\n",
        "            goal_selection_strategy=params[\"goal_selection_strategy\"],\n",
        "            verbose=params[\"verbose\"],\n",
        "            tensorboard_log=params[\"tensorboard_log\"],\n",
        "            buffer_size=params[\"buffer_size\"],\n",
        "            actor_lr=params[\"actor_lr\"],\n",
        "            critic_lr=params[\"critic_lr\"],\n",
        "            action_noise=action_noise,\n",
        "            gamma=params[\"gamma\"],\n",
        "            batch_size=params[\"batch_size\"],\n",
        "            policy_kwargs=dict(layers=[256, 256, 256]))\n",
        "\n",
        "print(\"start training\")\n",
        "model.learn(params[\"steps\"], tb_log_name=params[\"tb_log_name\"])\n",
        "model.save(params[\"filename\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej6aSTB9sOn7",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-9OFf5gYslQ",
        "colab_type": "text"
      },
      "source": [
        "helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIjaBTN7YqVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\"><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6bVQyBWvlb",
        "colab_type": "text"
      },
      "source": [
        "playing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iNpyslhsEIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"parking-v0\")\n",
        "model = HER.load('her_ddpg_highway', env=env)\n",
        "\n",
        "env = wrap_env(env)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "# Evaluate the agent\n",
        "episode_reward = 0\n",
        "for _ in range(500):\n",
        "\taction, _ = model.predict(obs)\n",
        "\tobs, reward, done, info = env.step(action)\n",
        "\tepisode_reward += reward\n",
        "\tif done or info.get('is_success', False):\n",
        "\t\tprint(\"Reward:\", episode_reward, \"Success?\", info.get('is_success', False))\n",
        "\t\tepisode_reward = 0.0\n",
        "\t\tobs = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}